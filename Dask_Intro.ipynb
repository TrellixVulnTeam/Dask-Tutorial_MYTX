{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:50px\">Dask Introduction</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask natively scales Python. It provides advanced parallelism for analytics, enabling performance at scale for the tools you love.\n",
    "\n",
    "- Built in Python \n",
    "- Scales properly from single laptops to 1000-node clusters\n",
    "- Leverages with existing Python APIs as mush as possible\n",
    "\n",
    "**Please Note:**\n",
    "  - Explicit is better than implicit\n",
    "  - Simple is better than complex\n",
    "  - Complex is better than complicated\n",
    "  - Readability counts\n",
    "  - If the implementation is hard to explain, it is a bad idea\n",
    "  - If the implementation is easy to explain, it may be a good idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask was created in 2014 as part of the Blaze project, a DARPA funded project at Continuum/Anaconda. It has since grown into a multi-institution community project with developers from projects including Numpy, Pandas, Jupter and Scikit-Learn. Many of the core Dask maintainers are employed to work on the project by companies including Continuum/Anaconda, Prefect, NVIDIA, Captial One, Saturn Cloud and Coiled.\n",
    "\n",
    "Fundamentally, Dask allows a variety of parallel workflows using existing Python constructs, patterns, or libraries, including dataframes, arrays (Scaling out Numpy), bags (an unordered collection construct a bit like `Counter`), and `concurrent.futures`.\n",
    "\n",
    "In addition to working in conjunction with Python ecosystem tools, Dask's extremely low scheduling overhead (nanoseconds in some cases) allows it work well even on single machines, and smoothly scale up.\n",
    "\n",
    "Dask support a variety of use cases for industry and research: https://stories.dask.org/en/latest/\n",
    "\n",
    "With its recent 2.x releases, and integration to other projects (e.g., RAPIDS for GPU computation), many commercial enterprise are paying attention and jumping in to parallel Python with Dask.\n",
    "\n",
    "__Dask Ecosystem__\n",
    "\n",
    "In addition to the core Dask library and its Distributed scheduler, then Dask ecosystem connects several additional initiatives, including...\n",
    "* Dask ML - parallel machine learning, with a scikit-learn-style API\n",
    "* Dask-kubernetes\n",
    "* Dask-XGboost\n",
    "* Dask-YARN\n",
    "* Dask-image\n",
    "* Dask-cuDF\n",
    "* ... \n",
    "\n",
    "__What's Not Part of Dask?__\n",
    "\n",
    "There are lots of functions that integrate to Dask, but are not represented in the core Dask ecosystem, including...\n",
    "* a SQL engine\n",
    "* data storage\n",
    "* data catalog\n",
    "* visualization\n",
    "* coarse-grained scheduling / orchestration\n",
    "* streaming\n",
    "\n",
    "although there are typically other Python packages that fill these needs (e.g. Kartothek or Intake for a data catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Do We Set Up and/or Deploy Dask?\n",
    "\n",
    "The easiest way to install Dask is with Anaconda: `conda install dask`\n",
    "    \n",
    "__Schedulers and Clustering__\n",
    "\n",
    "Dask has a simple default scheduler called the \"single machine scheduler\" -- this is the scheduler that is used if your `import dask` and start running code without explicitly using a `Client` object. It can be handy for quick-and-dirty testing, but I would (*warning! opinion!*) suggest that a best practice is to __use the newer \"distributed scheduler\" even for single-machine workloads__.\n",
    "\n",
    "The distributed scheduler can work with\n",
    "* threads (although that is often not a great idea due to the GIL) in one process\n",
    "* multiple processes on one machine\n",
    "* multiple processes on multiple machines\n",
    "\n",
    "The distributed scheduler has addtional useful features including data locality awareness and realtime graphical dashboards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's See Some Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, let's take a look at one particular, common use case for Dask: scaling Pandas dataframes to\n",
    "- larger datasets (which don't fit in memory) and\n",
    "- multiple processes (which could be on multiple nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed comparison for small dataset (2MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed comparison for big dataset (250MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There will be ___ partitions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Return the number of rows for each country in Aisa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: \"compute\" really run the work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are writing counterparts to read methods which we can use:\n",
    "    \n",
    "- read_csv \\ to_csv\n",
    "- read_hdf \\ to_hdf\n",
    "- read_json \\ to_json\n",
    "- read_parquet \\ to_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best practice:\n",
    "\n",
    "HDF5 is a popular choice for Pandas users with high performance needs. We encourage Dask DataFrame users to store and load data using **Parquet** instead. Apache Parquet is a columnar binary format that is easy to split into multiple files (easier for parallel loading) and is generally much simpler to deal with than HDF5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask",
   "language": "python",
   "name": "dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
